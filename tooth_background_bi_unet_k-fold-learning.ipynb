{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow-gpu==2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall -y tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pydot-ng \n",
    "# !pip install graphviz \n",
    "# !pip install pydot==1.2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select GPU for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import request package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model import *\n",
    "from data import *\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold-learning hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 6\n",
    "#K-fold default\n",
    "\n",
    "train_image_folder = '/home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/dataset/train_tooth/train/image'\n",
    "train_label_folder = '/home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/dataset/train_tooth/train/label'\n",
    "k_fold_data_save_path = '/home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/dataset/train_tooth'\n",
    "k_fold_folder_name = 'k_fold_learning'\n",
    "#load data and split K group\n",
    "\n",
    "model_save_path = '/home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/'\n",
    "model_save_name = 'tooth_background_bi_k-fold'\n",
    "model_save_tpye = '.hdf5'\n",
    "# model will be save in 'model_save_path+model_save_name_{K}+model_save_tpye\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold folder build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_fold_learning create finish!!\n"
     ]
    }
   ],
   "source": [
    "# if show create finish!! can keep going next step\n",
    "k_fold_folder_path = os.path.join(k_fold_data_save_path, k_fold_folder_name)\n",
    "\n",
    "if not os.path.isdir(k_fold_folder_path):\n",
    "    try:\n",
    "        os.makedirs(k_fold_folder_path)\n",
    "        os.makedirs(model_save_path)\n",
    "        for i in range(k):\n",
    "            os.makedirs(os.path.join(k_fold_folder_path,'group{}'.format(i+1)))\n",
    "            os.makedirs(os.path.join(k_fold_folder_path,'group{}/image'.format(i+1)))\n",
    "            os.makedirs(os.path.join(k_fold_folder_path,'group{}/label'.format(i+1)))\n",
    "            os.makedirs(os.path.join(k_fold_folder_path,'group{}/aug'.format(i+1)))\n",
    "            os.makedirs(os.path.join(k_fold_folder_path,'group{}/skip_data'.format(i+1)))\n",
    "            os.makedirs(os.path.join(k_fold_folder_path,'group{}/skip_data/image'.format(i+1)))\n",
    "            os.makedirs(os.path.join(k_fold_folder_path,'group{}/skip_data/label'.format(i+1)))\n",
    "            os.makedirs(os.path.join(k_fold_folder_path,'group{}/skip_data/prediction'.format(i+1)))\n",
    "            \n",
    "        print('{} create finish!!'.format(k_fold_folder_name))\n",
    "    except:\n",
    "        print('{} build error'.format(k_fold_folder_name))\n",
    "        os.rmdir(k_fold_folder_path)\n",
    "        os.rmdir(model_save_path)\n",
    "else:\n",
    "    print('{} already exist'.format(k_fold_folder_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign data in k group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:00<00:00, 332.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data in 6 groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "traing_image_list = glob.glob(train_image_folder+'/*.PNG')\n",
    "#traing_image_list.sort()\n",
    "random.shuffle (traing_image_list)\n",
    "\n",
    "total_data_len = len(traing_image_list)\n",
    "batch_num = total_data_len/k\n",
    "\n",
    "for index,img_path in enumerate(tqdm(traing_image_list, total = total_data_len)):\n",
    "    #data in other group \n",
    "    group_list = [i+1 for i in range(k)]\n",
    "    group_list.pop(int(index/batch_num))\n",
    "    for group in group_list:\n",
    "        img_dst = os.path.join(k_fold_folder_path,'group{}/image'.format(group))\n",
    "        shutil.copy2(img_path,img_dst)\n",
    "        label_path = img_path.replace(train_image_folder,train_label_folder)\n",
    "        label_dst = img_dst.replace('image','label')\n",
    "        shutil.copy2(label_path,label_dst)\n",
    "    \n",
    "    \n",
    "    #put data in skip_data path \n",
    "    skip_img_dst = os.path.join(k_fold_folder_path,'group{}/skip_data/image'.format(int(index/batch_num)+1))\n",
    "    shutil.copy2(img_path,skip_img_dst)\n",
    "    skip_label_path = img_path.replace(train_image_folder,train_label_folder)\n",
    "    skip_label_dst = skip_img_dst.replace('image','label')\n",
    "    shutil.copy2(skip_label_path,skip_label_dst)\n",
    "print('Data in {} groups'.format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation in k group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_augmentation(train_parent_folder,aug_path):\n",
    "    data_gen_args = dict(rotation_range=0.2,\n",
    "                        width_shift_range=0.05,\n",
    "                        height_shift_range=0.05,\n",
    "                        shear_range=0.05,\n",
    "                        zoom_range=0.05,\n",
    "                        horizontal_flip=True,\n",
    "                        fill_mode='nearest')\n",
    "    myGenerator = trainGenerator(20,train_parent_folder,'image','label',data_gen_args,save_to_dir = aug_path)\n",
    "    \n",
    "    num_batch = 3\n",
    "    for i,batch in enumerate(myGenerator):\n",
    "        if(i >= num_batch):\n",
    "            break\n",
    "    #image_arr,mask_arr = geneTrainNpy(aug_path,aug_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is group1\n",
      "Found 56 images belonging to 1 classes.\n",
      "Found 56 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:02<00:11,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is group2\n",
      "Found 57 images belonging to 1 classes.\n",
      "Found 57 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:04<00:08,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is group3\n",
      "Found 57 images belonging to 1 classes.\n",
      "Found 57 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:06<00:06,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is group4\n",
      "Found 56 images belonging to 1 classes.\n",
      "Found 56 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:08<00:04,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is group5\n",
      "Found 57 images belonging to 1 classes.\n",
      "Found 57 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:11<00:02,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is group6\n",
      "Found 57 images belonging to 1 classes.\n",
      "Found 57 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:13<00:00,  2.26s/it]\n"
     ]
    }
   ],
   "source": [
    "k_fold_folder_path = os.path.join(k_fold_data_save_path, k_fold_folder_name)\n",
    "group_list = os.listdir(k_fold_folder_path)\n",
    "group_list.sort()\n",
    "\n",
    "for group_name in tqdm(group_list, total = len(group_list)):\n",
    "    print('This is {}'.format(group_name))\n",
    "    group_path = os.path.join(k_fold_folder_path, group_name)\n",
    "    aug_path = os.path.join(group_path, 'aug')\n",
    "    Data_augmentation(group_path, aug_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Unet with training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_in_traindata_unet(train_parent_folder,model_save_path, model_checkpoint):\n",
    "    data_gen_args = dict(rotation_range=0.2,\n",
    "                        width_shift_range=0.05,\n",
    "                        height_shift_range=0.05,\n",
    "                        shear_range=0.05,\n",
    "                        zoom_range=0.05,\n",
    "                        horizontal_flip=True,\n",
    "                        fill_mode='nearest')\n",
    "    myGene = trainGenerator(3,train_parent_folder,'image','label',data_gen_args,save_to_dir = None)\n",
    "    model = unet()\n",
    "    model.fit_generator(myGene,steps_per_epoch=2000,epochs=5,callbacks=[model_checkpoint])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Unet with augmentation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_in_augdata_unet(model, aug_path, model_save_path, model_checkpoint):    \n",
    "    imgs_train,imgs_mask_train = geneTrainNpy(aug_path, aug_path)\n",
    "    model.fit(imgs_train, imgs_mask_train, batch_size=2, nb_epoch=5, verbose=1,validation_split=0.2, shuffle=True, callbacks=[model_checkpoint])\n",
    "    model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model start training in group1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/unet/model.py:56: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n",
      "  model = Model(input = inputs, output = conv10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Found 56 images belonging to 1 classes.\n",
      "Found 56 images belonging to 1 classes.\n",
      "2000/2000 [==============================] - 165s 83ms/step - loss: 0.4473 - accuracy: 0.8427\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.44737, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group1.hdf5\n",
      "Epoch 2/5\n",
      "2000/2000 [==============================] - 157s 78ms/step - loss: 0.2967 - accuracy: 0.9537\n",
      "\n",
      "Epoch 00002: loss improved from 0.44737 to 0.29680, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group1.hdf5\n",
      "Epoch 3/5\n",
      "2000/2000 [==============================] - 157s 78ms/step - loss: 0.2456 - accuracy: 0.9694\n",
      "\n",
      "Epoch 00003: loss improved from 0.29680 to 0.24567, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group1.hdf5\n",
      "Epoch 4/5\n",
      "2000/2000 [==============================] - 156s 78ms/step - loss: 0.2017 - accuracy: 0.9818\n",
      "\n",
      "Epoch 00004: loss improved from 0.24567 to 0.20166, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group1.hdf5\n",
      "Epoch 5/5\n",
      "2000/2000 [==============================] - 156s 78ms/step - loss: 0.1768 - accuracy: 0.9830\n",
      "\n",
      "Epoch 00005: loss improved from 0.20166 to 0.17677, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group1.hdf5\n",
      "Train on 60 samples, validate on 16 samples\n",
      "Epoch 1/5\n",
      " 2/60 [>.............................] - ETA: 1s - loss: 0.2819 - accuracy: 0.9590"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 2s 35ms/step - loss: 0.2427 - accuracy: 0.9453 - val_loss: 0.2219 - val_accuracy: 0.9522\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.17677\n",
      "Epoch 2/5\n",
      "60/60 [==============================] - 2s 32ms/step - loss: 0.2211 - accuracy: 0.9529 - val_loss: 0.2284 - val_accuracy: 0.9439\n",
      "\n",
      "Epoch 00002: loss did not improve from 0.17677\n",
      "Epoch 3/5\n",
      "60/60 [==============================] - 2s 32ms/step - loss: 0.2084 - accuracy: 0.9573 - val_loss: 0.2008 - val_accuracy: 0.9602\n",
      "\n",
      "Epoch 00003: loss did not improve from 0.17677\n",
      "Epoch 4/5\n",
      "60/60 [==============================] - 2s 32ms/step - loss: 0.1867 - accuracy: 0.9710 - val_loss: 0.2068 - val_accuracy: 0.9598\n",
      "\n",
      "Epoch 00004: loss did not improve from 0.17677\n",
      "Epoch 5/5\n",
      "60/60 [==============================] - 2s 32ms/step - loss: 0.1785 - accuracy: 0.9751 - val_loss: 0.1911 - val_accuracy: 0.9680\n",
      "\n",
      "Epoch 00005: loss did not improve from 0.17677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [13:30<1:07:33, 810.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model start training in group2\n",
      "Epoch 1/5\n",
      "Found 57 images belonging to 1 classes.\n",
      "Found 57 images belonging to 1 classes.\n",
      "2000/2000 [==============================] - 160s 80ms/step - loss: 0.4535 - accuracy: 0.8376\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.45350, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group2.hdf5\n",
      "Epoch 2/5\n",
      "2000/2000 [==============================] - 159s 79ms/step - loss: 0.3033 - accuracy: 0.9520\n",
      "\n",
      "Epoch 00002: loss improved from 0.45350 to 0.30328, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group2.hdf5\n",
      "Epoch 3/5\n",
      "2000/2000 [==============================] - 159s 79ms/step - loss: 0.2442 - accuracy: 0.9729\n",
      "\n",
      "Epoch 00003: loss improved from 0.30328 to 0.24425, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group2.hdf5\n",
      "Epoch 4/5\n",
      "2000/2000 [==============================] - 159s 79ms/step - loss: 0.2081 - accuracy: 0.9800\n",
      "\n",
      "Epoch 00004: loss improved from 0.24425 to 0.20814, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group2.hdf5\n",
      "Epoch 5/5\n",
      "2000/2000 [==============================] - 158s 79ms/step - loss: 0.0907 - accuracy: 0.9839\n",
      "\n",
      "Epoch 00005: loss improved from 0.20814 to 0.09070, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group2.hdf5\n",
      "Train on 61 samples, validate on 16 samples\n",
      "Epoch 1/5\n",
      "61/61 [==============================] - 3s 51ms/step - loss: 0.0916 - accuracy: 0.9664 - val_loss: 0.1074 - val_accuracy: 0.9589\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.09070\n",
      "Epoch 2/5\n",
      "61/61 [==============================] - 2s 32ms/step - loss: 0.0975 - accuracy: 0.9641 - val_loss: 0.1194 - val_accuracy: 0.9543\n",
      "\n",
      "Epoch 00002: loss did not improve from 0.09070\n",
      "Epoch 3/5\n",
      "61/61 [==============================] - 2s 32ms/step - loss: 0.0938 - accuracy: 0.9634 - val_loss: 0.1173 - val_accuracy: 0.9527\n",
      "\n",
      "Epoch 00003: loss did not improve from 0.09070\n",
      "Epoch 4/5\n",
      "61/61 [==============================] - 2s 32ms/step - loss: 0.0564 - accuracy: 0.9774 - val_loss: 0.0918 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00004: loss improved from 0.09070 to 0.05643, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group2.hdf5\n",
      "Epoch 5/5\n",
      "61/61 [==============================] - 2s 32ms/step - loss: 0.0411 - accuracy: 0.9829 - val_loss: 0.0773 - val_accuracy: 0.9710\n",
      "\n",
      "Epoch 00005: loss improved from 0.05643 to 0.04112, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group2.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [27:06<54:08, 812.15s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model start training in group3\n",
      "Epoch 1/5\n",
      "Found 57 images belonging to 1 classes.\n",
      "Found 57 images belonging to 1 classes.\n",
      "2000/2000 [==============================] - 161s 80ms/step - loss: 0.3878 - accuracy: 0.8383\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.38782, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group3.hdf5\n",
      "Epoch 2/5\n",
      "2000/2000 [==============================] - 159s 79ms/step - loss: 0.0848 - accuracy: 0.9636\n",
      "\n",
      "Epoch 00002: loss improved from 0.38782 to 0.08478, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group3.hdf5\n",
      "Epoch 3/5\n",
      "2000/2000 [==============================] - 159s 79ms/step - loss: 0.0532 - accuracy: 0.9774\n",
      "\n",
      "Epoch 00003: loss improved from 0.08478 to 0.05316, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group3.hdf5\n",
      "Epoch 4/5\n",
      "2000/2000 [==============================] - 159s 79ms/step - loss: 0.0369 - accuracy: 0.9842\n",
      "\n",
      "Epoch 00004: loss improved from 0.05316 to 0.03694, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group3.hdf5\n",
      "Epoch 5/5\n",
      "2000/2000 [==============================] - 159s 79ms/step - loss: 0.0275 - accuracy: 0.9881\n",
      "\n",
      "Epoch 00005: loss improved from 0.03694 to 0.02751, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group3.hdf5\n",
      "Train on 61 samples, validate on 16 samples\n",
      "Epoch 1/5\n",
      "61/61 [==============================] - 2s 34ms/step - loss: 0.1169 - accuracy: 0.9592 - val_loss: 0.1425 - val_accuracy: 0.9415\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.02751\n",
      "Epoch 2/5\n",
      "61/61 [==============================] - 2s 32ms/step - loss: 0.1615 - accuracy: 0.9383 - val_loss: 0.1390 - val_accuracy: 0.9458\n",
      "\n",
      "Epoch 00002: loss did not improve from 0.02751\n",
      "Epoch 3/5\n",
      "61/61 [==============================] - 2s 32ms/step - loss: 0.1225 - accuracy: 0.9521 - val_loss: 0.1453 - val_accuracy: 0.9434\n",
      "\n",
      "Epoch 00003: loss did not improve from 0.02751\n",
      "Epoch 4/5\n",
      "61/61 [==============================] - 2s 32ms/step - loss: 0.0966 - accuracy: 0.9618 - val_loss: 0.1305 - val_accuracy: 0.9506\n",
      "\n",
      "Epoch 00004: loss did not improve from 0.02751\n",
      "Epoch 5/5\n",
      "61/61 [==============================] - 2s 32ms/step - loss: 0.0558 - accuracy: 0.9768 - val_loss: 0.0865 - val_accuracy: 0.9654\n",
      "\n",
      "Epoch 00005: loss did not improve from 0.02751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [40:39<40:37, 812.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model start training in group4\n",
      "Epoch 1/5\n",
      "Found 56 images belonging to 1 classes.\n",
      "Found 56 images belonging to 1 classes.\n",
      "2000/2000 [==============================] - 159s 79ms/step - loss: 0.4408 - accuracy: 0.8505\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.44073, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group4.hdf5\n",
      "Epoch 2/5\n",
      "2000/2000 [==============================] - 157s 79ms/step - loss: 0.2961 - accuracy: 0.9522\n",
      "\n",
      "Epoch 00002: loss improved from 0.44073 to 0.29614, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group4.hdf5\n",
      "Epoch 3/5\n",
      "2000/2000 [==============================] - 157s 78ms/step - loss: 0.2423 - accuracy: 0.9698\n",
      "\n",
      "Epoch 00003: loss improved from 0.29614 to 0.24230, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group4.hdf5\n",
      "Epoch 4/5\n",
      "2000/2000 [==============================] - 157s 78ms/step - loss: 0.1995 - accuracy: 0.9814\n",
      "\n",
      "Epoch 00004: loss improved from 0.24230 to 0.19949, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group4.hdf5\n",
      "Epoch 5/5\n",
      "2000/2000 [==============================] - 157s 79ms/step - loss: 0.1727 - accuracy: 0.9841\n",
      "\n",
      "Epoch 00005: loss improved from 0.19949 to 0.17261, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group4.hdf5\n",
      "Train on 60 samples, validate on 16 samples\n",
      "Epoch 1/5\n",
      "60/60 [==============================] - 2s 34ms/step - loss: 0.1869 - accuracy: 0.9723 - val_loss: 0.1971 - val_accuracy: 0.9675\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.17261\n",
      "Epoch 2/5\n",
      "60/60 [==============================] - 2s 32ms/step - loss: 0.1839 - accuracy: 0.9719 - val_loss: 0.2741 - val_accuracy: 0.9290\n",
      "\n",
      "Epoch 00002: loss did not improve from 0.17261\n",
      "Epoch 3/5\n",
      "60/60 [==============================] - 2s 32ms/step - loss: 0.2721 - accuracy: 0.9312 - val_loss: 0.2885 - val_accuracy: 0.9054\n",
      "\n",
      "Epoch 00003: loss did not improve from 0.17261\n",
      "Epoch 4/5\n",
      "60/60 [==============================] - 2s 32ms/step - loss: 0.2501 - accuracy: 0.9338 - val_loss: 0.2402 - val_accuracy: 0.9388\n",
      "\n",
      "Epoch 00004: loss did not improve from 0.17261\n",
      "Epoch 5/5\n",
      "60/60 [==============================] - 2s 32ms/step - loss: 0.2005 - accuracy: 0.9618 - val_loss: 0.2047 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00005: loss did not improve from 0.17261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [54:04<27:00, 810.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model start training in group5\n",
      "Epoch 1/5\n",
      "Found 57 images belonging to 1 classes.\n",
      "Found 57 images belonging to 1 classes.\n",
      "2000/2000 [==============================] - 160s 80ms/step - loss: 0.2150 - accuracy: 0.9035\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.21496, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group5.hdf5\n",
      "Epoch 2/5\n",
      "2000/2000 [==============================] - 158s 79ms/step - loss: 0.0736 - accuracy: 0.9685\n",
      "\n",
      "Epoch 00002: loss improved from 0.21496 to 0.07362, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group5.hdf5\n",
      "Epoch 3/5\n",
      "2000/2000 [==============================] - 159s 79ms/step - loss: 0.0486 - accuracy: 0.9793\n",
      "\n",
      "Epoch 00003: loss improved from 0.07362 to 0.04857, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group5.hdf5\n",
      "Epoch 4/5\n",
      "2000/2000 [==============================] - 159s 79ms/step - loss: 0.0316 - accuracy: 0.9864\n",
      "\n",
      "Epoch 00004: loss improved from 0.04857 to 0.03162, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group5.hdf5\n",
      "Epoch 5/5\n",
      "2000/2000 [==============================] - 159s 79ms/step - loss: 0.0269 - accuracy: 0.9884\n",
      "\n",
      "Epoch 00005: loss improved from 0.03162 to 0.02695, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group5.hdf5\n",
      "Train on 61 samples, validate on 16 samples\n",
      "Epoch 1/5\n",
      "61/61 [==============================] - 2s 35ms/step - loss: 0.0850 - accuracy: 0.9685 - val_loss: 0.1484 - val_accuracy: 0.9445\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.02695\n",
      "Epoch 2/5\n",
      "61/61 [==============================] - 2s 32ms/step - loss: 0.1296 - accuracy: 0.9511 - val_loss: 0.1693 - val_accuracy: 0.9372\n",
      "\n",
      "Epoch 00002: loss did not improve from 0.02695\n",
      "Epoch 3/5\n",
      "61/61 [==============================] - 2s 32ms/step - loss: 0.1674 - accuracy: 0.9403 - val_loss: 0.1999 - val_accuracy: 0.9197\n",
      "\n",
      "Epoch 00003: loss did not improve from 0.02695\n",
      "Epoch 4/5\n",
      "61/61 [==============================] - 2s 32ms/step - loss: 0.1315 - accuracy: 0.9474 - val_loss: 0.1468 - val_accuracy: 0.9416\n",
      "\n",
      "Epoch 00004: loss did not improve from 0.02695\n",
      "Epoch 5/5\n",
      "61/61 [==============================] - 2s 32ms/step - loss: 0.0991 - accuracy: 0.9616 - val_loss: 0.1426 - val_accuracy: 0.9449\n",
      "\n",
      "Epoch 00005: loss did not improve from 0.02695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [1:07:37<13:31, 811.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model start training in group6\n",
      "Epoch 1/5\n",
      "Found 57 images belonging to 1 classes.\n",
      "Found 57 images belonging to 1 classes.\n",
      "2000/2000 [==============================] - 160s 80ms/step - loss: 0.6904 - accuracy: 0.5561\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.69037, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group6.hdf5\n",
      "Epoch 2/5\n",
      "2000/2000 [==============================] - 159s 79ms/step - loss: 0.6877 - accuracy: 0.5561\n",
      "\n",
      "Epoch 00002: loss improved from 0.69037 to 0.68766, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group6.hdf5\n",
      "Epoch 3/5\n",
      "2000/2000 [==============================] - 159s 79ms/step - loss: 0.6870 - accuracy: 0.5562\n",
      "\n",
      "Epoch 00003: loss improved from 0.68766 to 0.68698, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group6.hdf5\n",
      "Epoch 4/5\n",
      "2000/2000 [==============================] - 159s 79ms/step - loss: 0.6869 - accuracy: 0.5560\n",
      "\n",
      "Epoch 00004: loss improved from 0.68698 to 0.68689, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group6.hdf5\n",
      "Epoch 5/5\n",
      "2000/2000 [==============================] - 159s 79ms/step - loss: 0.6869 - accuracy: 0.5560\n",
      "\n",
      "Epoch 00005: loss improved from 0.68689 to 0.68688, saving model to /home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/tooth_background_bi_k-fold/tooth_background_bi_k-fold_group6.hdf5\n",
      "Train on 61 samples, validate on 16 samples\n",
      "Epoch 1/5\n",
      "61/61 [==============================] - 2s 35ms/step - loss: 0.6882 - accuracy: 0.5498 - val_loss: 0.6889 - val_accuracy: 0.5468\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.68688\n",
      "Epoch 2/5\n",
      "61/61 [==============================] - 2s 32ms/step - loss: 0.6882 - accuracy: 0.5498 - val_loss: 0.6889 - val_accuracy: 0.5468\n",
      "\n",
      "Epoch 00002: loss did not improve from 0.68688\n",
      "Epoch 3/5\n",
      "61/61 [==============================] - 2s 32ms/step - loss: 0.6882 - accuracy: 0.5498 - val_loss: 0.6889 - val_accuracy: 0.5468\n",
      "\n",
      "Epoch 00003: loss did not improve from 0.68688\n",
      "Epoch 4/5\n",
      "61/61 [==============================] - 2s 32ms/step - loss: 0.6882 - accuracy: 0.5498 - val_loss: 0.6889 - val_accuracy: 0.5468\n",
      "\n",
      "Epoch 00004: loss did not improve from 0.68688\n",
      "Epoch 5/5\n",
      "61/61 [==============================] - 2s 32ms/step - loss: 0.6882 - accuracy: 0.5498 - val_loss: 0.6889 - val_accuracy: 0.5468\n",
      "\n",
      "Epoch 00005: loss did not improve from 0.68688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [1:21:12<00:00, 812.00s/it]\n"
     ]
    }
   ],
   "source": [
    "k_fold_folder_path = os.path.join(k_fold_data_save_path, k_fold_folder_name)\n",
    "group_list = os.listdir(k_fold_folder_path)\n",
    "group_list.sort()\n",
    "\n",
    "for group_name in tqdm(group_list, total = len(group_list)):\n",
    "    print('Model start training in {}'.format(group_name))\n",
    "    group_path = os.path.join(k_fold_folder_path, group_name)\n",
    "    aug_path = os.path.join(group_path, 'aug')\n",
    "    model_save_total_path = model_save_path+model_save_name+'_{}'.format(group_name)+model_save_tpye\n",
    "    model_checkpoint = ModelCheckpoint(model_save_total_path, monitor='loss',verbose=1, save_best_only=True)\n",
    "    model = train_in_traindata_unet(group_path, model_save_total_path, model_checkpoint)\n",
    "    train_in_augdata_unet(model, aug_path, model_save_total_path, model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test your model and save predicted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import *\n",
    "from data import *\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_bin(img, model_list, flip_ = False):\n",
    "    ensemble_final_result = np.zeros((img.shape[0], img.shape[1],1), dtype = 'float32')\n",
    "    for index, model in enumerate(model_list):\n",
    "        pr_img = model.predict(img)\n",
    "        sqpr_img = np.squeeze(pr_img)\n",
    "        ensemble_img = sqpr_img.copy()\n",
    "        if flip_:\n",
    "            imgp = np.squeeze(img)\n",
    "            h_flip = cv2.flip(imgp, 1)\n",
    "            h_flip = np.expand_dims(h_flip, axis=(0,3))\n",
    "            pre_h_flip = model.predict(h_flip)\n",
    "            pre_h_flip = np.squeeze(pre_h_flip)\n",
    "            h_flip = cv2.flip(pre_h_flip, 1)\n",
    "\n",
    "            v_flip = cv2.flip(imgp, 0)\n",
    "            v_flip = np.expand_dims(v_flip, axis=(0,3))\n",
    "            pre_v_flip = model.predict(v_flip)\n",
    "            pre_v_flip = np.squeeze(pre_v_flip)\n",
    "            v_flip = cv2.flip(pre_v_flip, 0)\n",
    "\n",
    "            hv_flip = cv2.flip(imgp, -1)\n",
    "            hv_flip = np.expand_dims(hv_flip, axis=(0,3))\n",
    "            pre_hv_flip = model.predict(hv_flip)\n",
    "            pre_hv_flip = np.squeeze(pre_hv_flip)\n",
    "            hv_flip = cv2.flip(pre_hv_flip, -1)\n",
    "            ensemble_img = (sqpr_img+h_flip+v_flip+hv_flip)/4\n",
    "        ensemble_final_result = ensemble_final_result+ensemble_img\n",
    "    ensemble_final_result = ensemble_final_result/(index+1)\n",
    "    return np.expand_dims(np.where(ensemble_final_result > 0.5, 255, 0), axis=(0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import shutil\n",
    "\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test_k_group_in_skip_data(test_folder, result_folder, model_save_path):\n",
    "    model = unet()\n",
    "    model.load_weights(model_save_path)\n",
    "    model_list = [model]\n",
    "    model_fit_img_size = (256,256)\n",
    "    test_img_path_list = glob.glob(test_folder+'/*.PNG')\n",
    "    for img_path in tqdm(test_img_path_list, total = len(test_img_path_list)):\n",
    "        img = io.imread(img_path,as_gray = True)\n",
    "        targe_size = img.shape\n",
    "        img = trans.resize(img,model_fit_img_size)\n",
    "        img = np.expand_dims(img, axis=(0,3))\n",
    "        img_esemble = ensemble_bin(img,model_list,flip_ = True)\n",
    "        img_esemble = np.squeeze(img_esemble)\n",
    "        cv2.imwrite(result_folder+'/{}'.format(img_path.split('/')[-1]),img_esemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|▊         | 1/12 [00:00<00:03,  3.23it/s]\u001b[A\n",
      " 25%|██▌       | 3/12 [00:00<00:02,  4.24it/s]\u001b[A\n",
      " 42%|████▏     | 5/12 [00:00<00:01,  5.48it/s]\u001b[A\n",
      " 58%|█████▊    | 7/12 [00:00<00:00,  6.84it/s]\u001b[A\n",
      " 75%|███████▌  | 9/12 [00:00<00:00,  8.32it/s]\u001b[A\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.41it/s]\u001b[A\n",
      " 17%|█▋        | 1/6 [00:01<00:06,  1.40s/it]\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|▉         | 1/11 [00:00<00:02,  3.73it/s]\u001b[A\n",
      " 27%|██▋       | 3/11 [00:00<00:01,  4.85it/s]\u001b[A\n",
      " 45%|████▌     | 5/11 [00:00<00:00,  6.18it/s]\u001b[A\n",
      " 64%|██████▎   | 7/11 [00:00<00:00,  7.62it/s]\u001b[A\n",
      " 82%|████████▏ | 9/11 [00:00<00:00,  9.12it/s]\u001b[A\n",
      "100%|██████████| 11/11 [00:00<00:00, 12.74it/s]\u001b[A\n",
      " 33%|███▎      | 2/6 [00:02<00:05,  1.35s/it]\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|▉         | 1/11 [00:00<00:02,  3.59it/s]\u001b[A\n",
      " 27%|██▋       | 3/11 [00:00<00:01,  4.33it/s]\u001b[A\n",
      " 45%|████▌     | 5/11 [00:00<00:01,  5.58it/s]\u001b[A\n",
      " 64%|██████▎   | 7/11 [00:00<00:00,  6.98it/s]\u001b[A\n",
      " 82%|████████▏ | 9/11 [00:00<00:00,  8.48it/s]\u001b[A\n",
      "100%|██████████| 11/11 [00:00<00:00, 11.11it/s]\u001b[A\n",
      " 50%|█████     | 3/6 [00:04<00:04,  1.36s/it]\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|▊         | 1/12 [00:00<00:03,  3.47it/s]\u001b[A\n",
      " 25%|██▌       | 3/12 [00:00<00:01,  4.55it/s]\u001b[A\n",
      " 42%|████▏     | 5/12 [00:00<00:01,  5.81it/s]\u001b[A\n",
      " 58%|█████▊    | 7/12 [00:00<00:00,  7.21it/s]\u001b[A\n",
      " 75%|███████▌  | 9/12 [00:00<00:00,  8.64it/s]\u001b[A\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.57it/s]\u001b[A\n",
      " 67%|██████▋   | 4/6 [00:05<00:02,  1.36s/it]\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|▉         | 1/11 [00:00<00:04,  2.47it/s]\u001b[A\n",
      " 27%|██▋       | 3/11 [00:00<00:02,  3.31it/s]\u001b[A\n",
      " 45%|████▌     | 5/11 [00:00<00:01,  4.35it/s]\u001b[A\n",
      " 64%|██████▎   | 7/11 [00:00<00:00,  5.56it/s]\u001b[A\n",
      " 82%|████████▏ | 9/11 [00:00<00:00,  6.98it/s]\u001b[A\n",
      "100%|██████████| 11/11 [00:01<00:00, 10.83it/s]\u001b[A\n",
      " 83%|████████▎ | 5/6 [00:06<00:01,  1.37s/it]\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|▉         | 1/11 [00:00<00:02,  3.49it/s]\u001b[A\n",
      " 27%|██▋       | 3/11 [00:00<00:01,  4.58it/s]\u001b[A\n",
      " 45%|████▌     | 5/11 [00:00<00:01,  5.88it/s]\u001b[A\n",
      " 64%|██████▎   | 7/11 [00:00<00:00,  7.30it/s]\u001b[A\n",
      " 82%|████████▏ | 9/11 [00:00<00:00,  8.72it/s]\u001b[A\n",
      "100%|██████████| 11/11 [00:00<00:00, 12.39it/s]\u001b[A\n",
      "100%|██████████| 6/6 [00:08<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "k_fold_folder_path = os.path.join(k_fold_data_save_path, k_fold_folder_name)\n",
    "group_list = os.listdir(k_fold_folder_path)\n",
    "group_list.sort()\n",
    "\n",
    "for group_name in tqdm(group_list, total = len(group_list)):\n",
    "    test_folder = os.path.join(k_fold_folder_path, group_name, 'skip_data', 'image')\n",
    "    result_folder = os.path.join(k_fold_folder_path, group_name, 'skip_data', 'prediction')\n",
    "    model_save_total_path = model_save_path+model_save_name+'_{}'.format(group_name)+model_save_tpye\n",
    "    test_k_group_in_skip_data(test_folder, result_folder, model_save_total_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -y graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show testing result\n",
    "import os\n",
    "import cv2\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def show_k_fold_result(test_img_path_list):\n",
    "    for img_path in tqdm(test_img_path_list, total = len(test_img_path_list)):\n",
    "        img = cv2.imread(img_path,0)\n",
    "        predict_img = cv2.imread(img_path.replace('image','prediction'),0)\n",
    "        print(img.shape[::-1])\n",
    "        predict_img = cv2.resize(predict_img, img.shape[::-1][1:])\n",
    "        human_label = cv2.imread(img_path.replace('image','label'),0)\n",
    "        plt.figure(figsize = (20,20))\n",
    "        plt.subplot(131)\n",
    "        plt.title('original_test_img')\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.subplot(132)\n",
    "        plt.title('predict_test_img')\n",
    "        plt.imshow(predict_img, cmap='gray')\n",
    "        plt.subplot(133)\n",
    "        plt.title('human_label')\n",
    "        plt.imshow(human_label, cmap='gray')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(615,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "function takes exactly 2 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-3613a7cd349b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtest_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold_folder_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgroup_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'skip_data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'image'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtest_img_path_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_folder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/*.PNG'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mshow_k_fold_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_img_path_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-ead4214a5b88>\u001b[0m in \u001b[0;36mshow_k_fold_result\u001b[0;34m(test_img_path_list)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mpredict_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mpredict_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mhuman_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: function takes exactly 2 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "k_fold_folder_path = os.path.join(k_fold_data_save_path, k_fold_folder_name)\n",
    "group_list = os.listdir(k_fold_folder_path)\n",
    "group_list.sort()\n",
    "\n",
    "for group_name in tqdm(group_list, total = len(group_list)):\n",
    "    test_folder = os.path.join(k_fold_folder_path,group_name, 'skip_data', 'image')\n",
    "    test_img_path_list = glob.glob(test_folder+'/*.PNG')\n",
    "    show_k_fold_result(test_img_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|▊         | 1/12 [00:00<00:03,  3.31it/s]\u001b[A\n",
      " 42%|████▏     | 5/12 [00:00<00:01,  4.54it/s]\u001b[A\n",
      "100%|██████████| 12/12 [00:00<00:00, 19.45it/s][A\n",
      " 17%|█▋        | 1/6 [00:01<00:05,  1.00s/it]\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|▉         | 1/11 [00:00<00:02,  4.20it/s]\u001b[A\n",
      " 45%|████▌     | 5/11 [00:00<00:01,  5.70it/s]\u001b[A\n",
      "100%|██████████| 11/11 [00:00<00:00, 21.32it/s][A\n",
      " 33%|███▎      | 2/6 [00:01<00:03,  1.03it/s]\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|▉         | 1/11 [00:00<00:02,  3.91it/s]\u001b[A\n",
      " 27%|██▋       | 3/11 [00:00<00:01,  4.92it/s]\u001b[A\n",
      " 64%|██████▎   | 7/11 [00:00<00:00,  6.65it/s]\u001b[A\n",
      "100%|██████████| 11/11 [00:00<00:00, 17.20it/s]\u001b[A\n",
      " 50%|█████     | 3/6 [00:02<00:02,  1.01it/s]\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|▊         | 1/12 [00:00<00:02,  4.34it/s]\u001b[A\n",
      " 42%|████▏     | 5/12 [00:00<00:01,  5.89it/s]\u001b[A\n",
      "100%|██████████| 12/12 [00:00<00:00, 22.15it/s][A\n",
      " 67%|██████▋   | 4/6 [00:03<00:01,  1.03it/s]\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|▉         | 1/11 [00:00<00:03,  2.73it/s]\u001b[A\n",
      " 45%|████▌     | 5/11 [00:00<00:01,  3.76it/s]\u001b[A\n",
      "100%|██████████| 11/11 [00:00<00:00, 16.61it/s][A\n",
      " 83%|████████▎ | 5/6 [00:04<00:00,  1.00it/s]\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|▉         | 1/11 [00:00<00:02,  4.31it/s]\u001b[A\n",
      " 45%|████▌     | 5/11 [00:00<00:01,  5.86it/s]\u001b[A\n",
      "100%|██████████| 11/11 [00:00<00:00, 21.59it/s][A\n",
      "100%|██████████| 6/6 [00:05<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "k_fold_folder_path = os.path.join(k_fold_data_save_path, k_fold_folder_name)\n",
    "group_list = os.listdir(k_fold_folder_path)\n",
    "group_list.sort()\n",
    "\n",
    "for group_name in tqdm(group_list, total = len(group_list)):\n",
    "    test_folder = os.path.join(k_fold_folder_path, group_name, 'skip_data', 'image')\n",
    "    result_folder = os.path.join(k_fold_folder_path, group_name, 'skip_data', 'prediction')\n",
    "    model_save_total_path = model_save_path+model_save_name+'_{}'.format(group_name)+model_save_tpye\n",
    "    test_k_group_in_skip_data(test_folder, result_folder, model_save_total_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:01<00:00, 54.26it/s]\n"
     ]
    }
   ],
   "source": [
    "test_folder = '/home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/dataset/buccal_furcation_train/k_fold_learning/group5/image'\n",
    "result_folder = '/home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/dataset/buccal_furcation_train/k_fold_learning/group5/train_pre'\n",
    "model_save_total_path = '/home/jingping/aiforge/tooth_xray_segmention_for_test/unet_base/model/model_weight/k-fold/buccal_furcation_bi_unet_k-fold-learning_group5.hdf5'\n",
    "\n",
    "test_k_group_in_skip_data(test_folder, result_folder, model_save_total_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/usr/lib/python36.zip',\n",
       " '/usr/lib/python3.6',\n",
       " '/usr/lib/python3.6/lib-dynload',\n",
       " '',\n",
       " '/home/jingping/.local/lib/python3.6/site-packages',\n",
       " '/usr/local/lib/python3.6/dist-packages',\n",
       " '/usr/lib/python3/dist-packages',\n",
       " '/usr/local/lib/python3.6/dist-packages/IPython/extensions',\n",
       " '/home/jingping/.ipython']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
